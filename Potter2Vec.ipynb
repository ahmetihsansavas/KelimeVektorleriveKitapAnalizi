{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potter2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import pprint\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim.models.word2vec as w2v\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up logging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Download NLTK tokenizer models (only the first time)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ahmetihsan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ahmetihsan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metnin Hazırlanması"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kitaplar klasörden okunur**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_filenames = sorted(glob.glob(\"data/*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found books:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data/Atsiz.txt',\n",
       " 'data/book1.txt',\n",
       " 'data/book2.txt',\n",
       " 'data/book3.txt',\n",
       " 'data/book4.txt',\n",
       " 'data/book5.txt',\n",
       " 'data/book6.txt',\n",
       " 'data/book7.txt',\n",
       " 'data/sherlock.txt',\n",
       " 'data/sherlockfull.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Found books:\")\n",
    "book_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bütün kitaplar tek bir yerde birleştirilir**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 'data/Atsiz.txt'...\n",
      "Corpus is now 868589 characters long\n",
      "\n",
      "Reading 'data/book1.txt'...\n",
      "Corpus is now 1343008 characters long\n",
      "\n",
      "Reading 'data/book2.txt'...\n",
      "Corpus is now 1874663 characters long\n",
      "\n",
      "Reading 'data/book3.txt'...\n",
      "Corpus is now 2341624 characters long\n",
      "\n",
      "Reading 'data/book4.txt'...\n",
      "Corpus is now 3528866 characters long\n",
      "\n",
      "Reading 'data/book5.txt'...\n",
      "Corpus is now 3770350 characters long\n",
      "\n",
      "Reading 'data/book6.txt'...\n",
      "Corpus is now 4829355 characters long\n",
      "\n",
      "Reading 'data/book7.txt'...\n",
      "Corpus is now 5515892 characters long\n",
      "\n",
      "Reading 'data/sherlock.txt'...\n",
      "Corpus is now 6082099 characters long\n",
      "\n",
      "Reading 'data/sherlockfull.txt'...\n",
      "Corpus is now 9667369 characters long\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_raw = u\"\"\n",
    "for book_filename in book_filenames:\n",
    "    print(\"Reading '{0}'...\".format(book_filename))\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        corpus_raw += book_file.read()\n",
    "    print(\"Corpus is now {0} characters long\".format(len(corpus_raw)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLTK Kütüphanesi ile anlama etkisi olmayan kelimeler temizlenir**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sentences = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert into a list of words\n",
    "#rtemove unnnecessary,, split into words, no hyphens\n",
    "#list of words\n",
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence where each word is tokenized\n",
    "sentences = []\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book corpus contains 1,767,533 tokens\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print(\"The book corpus contains {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Oluşturulması ve Eğitilmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONCE we have vectors\n",
    "#step 3 - build model\n",
    "#3 main tasks that vectors help with\n",
    "#DISTANCE, SIMILARITY, RANKING\n",
    "\n",
    "# Dimensionality of the resulting word vectors.\n",
    "#more dimensions, more computationally expensive to train\n",
    "#but also more accurate\n",
    "#more dimensions = more generalized\n",
    "num_features = 300\n",
    "# Minimum word count threshold.\n",
    "min_word_count = 3\n",
    "\n",
    "# Number of threads to run in parallel.\n",
    "#more workers, faster we train\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "# Context window length.\n",
    "context_size = 7\n",
    "\n",
    "# Downsample setting for frequent words.\n",
    "#0 - 1e-5 is good for this\n",
    "downsampling = 1e-3\n",
    "\n",
    "# Seed for the RNG, to make the results reproducible.\n",
    "#random number generator\n",
    "#deterministic, good for debugging\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "potter2vec = w2v.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-02 18:56:10,952 : INFO : collecting all words and their counts\n",
      "2020-01-02 18:56:10,955 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-01-02 18:56:11,003 : INFO : PROGRESS: at sentence #10000, processed 125048 words, keeping 11228 word types\n",
      "2020-01-02 18:56:11,042 : INFO : PROGRESS: at sentence #20000, processed 280824 words, keeping 21123 word types\n",
      "2020-01-02 18:56:11,086 : INFO : PROGRESS: at sentence #30000, processed 449107 words, keeping 26345 word types\n",
      "2020-01-02 18:56:11,131 : INFO : PROGRESS: at sentence #40000, processed 622777 words, keeping 29935 word types\n",
      "2020-01-02 18:56:11,180 : INFO : PROGRESS: at sentence #50000, processed 812856 words, keeping 33237 word types\n",
      "2020-01-02 18:56:11,226 : INFO : PROGRESS: at sentence #60000, processed 993114 words, keeping 35881 word types\n",
      "2020-01-02 18:56:11,266 : INFO : PROGRESS: at sentence #70000, processed 1140477 words, keeping 39358 word types\n",
      "2020-01-02 18:56:11,320 : INFO : PROGRESS: at sentence #80000, processed 1346823 words, keeping 41043 word types\n",
      "2020-01-02 18:56:11,372 : INFO : PROGRESS: at sentence #90000, processed 1538880 words, keeping 43137 word types\n",
      "2020-01-02 18:56:11,428 : INFO : PROGRESS: at sentence #100000, processed 1706632 words, keeping 45015 word types\n",
      "2020-01-02 18:56:11,451 : INFO : collected 45666 word types from a corpus of 1767533 raw words and 103472 sentences\n",
      "2020-01-02 18:56:11,452 : INFO : Loading a fresh vocabulary\n",
      "2020-01-02 18:56:11,618 : INFO : effective_min_count=3 retains 21342 unique words (46% of original 45666, drops 24324)\n",
      "2020-01-02 18:56:11,618 : INFO : effective_min_count=3 leaves 1736613 word corpus (98% of original 1767533, drops 30920)\n",
      "2020-01-02 18:56:11,692 : INFO : deleting the raw counts dictionary of 45666 items\n",
      "2020-01-02 18:56:11,694 : INFO : sample=0.001 downsamples 47 most-common words\n",
      "2020-01-02 18:56:11,695 : INFO : downsampling leaves estimated 1374402 word corpus (79.1% of prior 1736613)\n",
      "2020-01-02 18:56:11,764 : INFO : estimated required memory for 21342 words and 300 dimensions: 61891800 bytes\n",
      "2020-01-02 18:56:11,765 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "potter2vec.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec vocabulary length: 21342\n"
     ]
    }
   ],
   "source": [
    "print(\"Word2Vec vocabulary length:\", len(potter2vec.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start training, this might take a minute or two...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2020-01-02 18:44:00,762 : INFO : training model with 4 workers on 21342 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=7\n",
      "2020-01-02 18:44:01,893 : INFO : EPOCH 1 - PROGRESS: at 15.25% examples, 164981 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-02 18:44:02,995 : INFO : EPOCH 1 - PROGRESS: at 29.01% examples, 166897 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-02 18:44:04,095 : INFO : EPOCH 1 - PROGRESS: at 40.92% examples, 160541 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-02 18:44:05,102 : INFO : EPOCH 1 - PROGRESS: at 51.61% examples, 162384 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-02 18:44:06,123 : INFO : EPOCH 1 - PROGRESS: at 61.18% examples, 158480 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-02 18:44:07,191 : INFO : EPOCH 1 - PROGRESS: at 73.08% examples, 153169 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-02 18:44:08,210 : INFO : EPOCH 1 - PROGRESS: at 81.54% examples, 151369 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-02 18:44:09,214 : INFO : EPOCH 1 - PROGRESS: at 92.82% examples, 151277 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-02 18:44:09,669 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-02 18:44:09,700 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-02 18:44:09,744 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-02 18:44:09,760 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-02 18:44:09,761 : INFO : EPOCH - 1 : training on 1767533 raw words (1374337 effective words) took 9.0s, 153017 effective words/s\n",
      "2020-01-02 18:44:10,795 : INFO : EPOCH 2 - PROGRESS: at 10.68% examples, 124356 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-02 18:44:11,896 : INFO : EPOCH 2 - PROGRESS: at 24.40% examples, 146499 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-02 18:44:12,979 : INFO : EPOCH 2 - PROGRESS: at 37.15% examples, 152662 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-02 18:44:13,982 : INFO : EPOCH 2 - PROGRESS: at 48.16% examples, 154780 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-02 18:44:15,052 : INFO : EPOCH 2 - PROGRESS: at 59.93% examples, 156796 words/s, in_qsize 6, out_qsize 1\n",
      "2020-01-02 18:44:16,062 : INFO : EPOCH 2 - PROGRESS: at 73.92% examples, 159101 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-02 18:44:17,082 : INFO : EPOCH 2 - PROGRESS: at 85.21% examples, 160563 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-02 18:44:18,102 : INFO : EPOCH 2 - PROGRESS: at 96.70% examples, 159961 words/s, in_qsize 6, out_qsize 0\n",
      "2020-01-02 18:44:18,151 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-02 18:44:18,271 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-02 18:44:18,291 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-02 18:44:18,292 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-02 18:44:18,293 : INFO : EPOCH - 2 : training on 1767533 raw words (1374739 effective words) took 8.5s, 161714 effective words/s\n",
      "2020-01-02 18:44:19,374 : INFO : EPOCH 3 - PROGRESS: at 12.87% examples, 142485 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-02 18:44:20,400 : INFO : EPOCH 3 - PROGRESS: at 23.82% examples, 143745 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-02 18:44:21,424 : INFO : EPOCH 3 - PROGRESS: at 35.45% examples, 148644 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-02 18:44:22,552 : INFO : EPOCH 3 - PROGRESS: at 46.70% examples, 147351 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-02 18:44:23,590 : INFO : EPOCH 3 - PROGRESS: at 58.26% examples, 150435 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-02 18:44:24,612 : INFO : EPOCH 3 - PROGRESS: at 71.83% examples, 152289 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-02 18:44:25,656 : INFO : EPOCH 3 - PROGRESS: at 78.70% examples, 147038 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-02 18:44:26,666 : INFO : EPOCH 3 - PROGRESS: at 89.63% examples, 147322 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-02 18:44:27,365 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-02 18:44:27,417 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-02 18:44:27,473 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-02 18:44:27,490 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-02 18:44:27,491 : INFO : EPOCH - 3 : training on 1767533 raw words (1374248 effective words) took 9.2s, 149721 effective words/s\n",
      "2020-01-02 18:44:28,513 : INFO : EPOCH 4 - PROGRESS: at 14.66% examples, 174904 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-02 18:44:29,572 : INFO : EPOCH 4 - PROGRESS: at 26.67% examples, 164124 words/s, in_qsize 6, out_qsize 1\n",
      "2020-01-02 18:44:30,669 : INFO : EPOCH 4 - PROGRESS: at 40.37% examples, 165895 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-02 18:44:31,810 : INFO : EPOCH 4 - PROGRESS: at 52.15% examples, 164916 words/s, in_qsize 7, out_qsize 1\n",
      "2020-01-02 18:44:32,889 : INFO : EPOCH 4 - PROGRESS: at 66.88% examples, 165679 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-02 18:44:33,928 : INFO : EPOCH 4 - PROGRESS: at 77.83% examples, 165804 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-02 18:44:34,935 : INFO : EPOCH 4 - PROGRESS: at 89.16% examples, 164656 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-02 18:44:35,660 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-02 18:44:35,688 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-02 18:44:35,719 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-02 18:44:35,787 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-02 18:44:35,787 : INFO : EPOCH - 4 : training on 1767533 raw words (1374392 effective words) took 8.3s, 165974 effective words/s\n",
      "2020-01-02 18:44:36,934 : INFO : EPOCH 5 - PROGRESS: at 15.25% examples, 162377 words/s, in_qsize 8, out_qsize 0\n",
      "2020-01-02 18:44:38,054 : INFO : EPOCH 5 - PROGRESS: at 29.01% examples, 164338 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-02 18:44:39,122 : INFO : EPOCH 5 - PROGRESS: at 40.36% examples, 158046 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-02 18:44:40,140 : INFO : EPOCH 5 - PROGRESS: at 50.62% examples, 158287 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-02 18:44:41,168 : INFO : EPOCH 5 - PROGRESS: at 61.62% examples, 159287 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-02 18:44:42,170 : INFO : EPOCH 5 - PROGRESS: at 75.23% examples, 160162 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-02 18:44:43,186 : INFO : EPOCH 5 - PROGRESS: at 86.04% examples, 160537 words/s, in_qsize 7, out_qsize 0\n",
      "2020-01-02 18:44:44,195 : INFO : EPOCH 5 - PROGRESS: at 98.37% examples, 161063 words/s, in_qsize 3, out_qsize 1\n",
      "2020-01-02 18:44:44,197 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-01-02 18:44:44,218 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-01-02 18:44:44,262 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-01-02 18:44:44,287 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-01-02 18:44:44,287 : INFO : EPOCH - 5 : training on 1767533 raw words (1374513 effective words) took 8.5s, 161988 effective words/s\n",
      "2020-01-02 18:44:44,288 : INFO : training on a 8837665 raw words (6872229 effective words) took 43.5s, 157910 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6872229, 8837665)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "potter2vec.train(sentences,total_examples=potter2vec.corpus_count, epochs=potter2vec.iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eğitilen modelin Kaydedilmesi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"trained\"):\n",
    "    os.makedirs(\"trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-02 18:44:44,322 : INFO : saving Word2Vec object under trained/potter2vec.w2v, separately None\n",
      "2020-01-02 18:44:44,325 : INFO : not storing attribute vectors_norm\n",
      "2020-01-02 18:44:44,328 : INFO : not storing attribute cum_table\n",
      "2020-01-02 18:44:44,941 : INFO : saved trained/potter2vec.w2v\n"
     ]
    }
   ],
   "source": [
    "potter2vec.save(os.path.join(\"trained\", \"potter2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaydedilen Modelin Yüklenmesi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-02 18:44:44,951 : INFO : loading Word2Vec object from trained/potter2vec.w2v\n",
      "2020-01-02 18:44:45,370 : INFO : loading wv recursively from trained/potter2vec.w2v.wv.* with mmap=None\n",
      "2020-01-02 18:44:45,372 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-01-02 18:44:45,378 : INFO : loading vocabulary recursively from trained/potter2vec.w2v.vocabulary.* with mmap=None\n",
      "2020-01-02 18:44:45,380 : INFO : loading trainables recursively from trained/potter2vec.w2v.trainables.* with mmap=None\n",
      "2020-01-02 18:44:45,380 : INFO : setting ignored attribute cum_table to None\n",
      "2020-01-02 18:44:45,382 : INFO : loaded trained/potter2vec.w2v\n"
     ]
    }
   ],
   "source": [
    "potter2vec = w2v.Word2Vec.load(os.path.join(\"trained\", \"potter2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSNE ile Eğitilen Modelin Görselleştirilmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my video - how to visualize a dataset easily\n",
    "tsne = sklearn.manifold.TSNE(n_components=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "all_word_vectors_matrix = potter2vec.wv.syn0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metinde geçen kelimelerin tsne ile koordinatlarının belirlenmesi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word_vectors_matrix_2d = tsne.fit_transform(all_word_vectors_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = pd.DataFrame(\n",
    "    [\n",
    "        (word, coords[0], coords[1])\n",
    "        for word, coords in [\n",
    "            (word, all_word_vectors_matrix_2d[potter2vec.wv.vocab[word].index])\n",
    "            for word in potter2vec.wv.vocab\n",
    "        ]\n",
    "    ],\n",
    "    columns=[\"word\", \"x\", \"y\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points.plot.scatter(\"x\", \"y\", s=10, figsize=(20, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_region(x_bounds, y_bounds):\n",
    "    slice = points[\n",
    "        (x_bounds[0] <= points.x) &\n",
    "        (points.x <= x_bounds[1]) & \n",
    "        (y_bounds[0] <= points.y) &\n",
    "        (points.y <= y_bounds[1])\n",
    "    ]\n",
    "    \n",
    "    ax = slice.plot.scatter(\"x\", \"y\", s=35, figsize=(10, 8))\n",
    "    for i, point in slice.iterrows():\n",
    "        ax.text(point.x + 0.005, point.y + 0.005, point.word, fontsize=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_region(x_bounds=(0.0, 5.2), y_bounds=(-0.5, -0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_region(x_bounds=(0, 1.25), y_bounds=(0, 1.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verilen kelimeler arasında anlamsal ilişkilerin keşfedilmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potter2vec.most_similar(\"Hogwarts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potter2vec.most_similar(\"Severus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potter2vec.most_similar(\"Lee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potter2vec.most_similar(\"Quidditch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potter2vec.most_similar(\"Potter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kelime çiftleri arasındaki pozitif-negatif ilişkinin keşfedilmesi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_similarity_cosmul(start1, end1, end2):\n",
    "    similarities = potter2vec.most_similar_cosmul(\n",
    "        positive=[end2, start1],\n",
    "        negative=[end1]\n",
    "    )\n",
    "    start2 = similarities[0][0]\n",
    "    print(\"{start1} is related to {end1}, as {start2} is related to {end2}\".format(**locals()))\n",
    "    return start2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_similarity_cosmul(\"Severus\", \"Minerva\", \"Ron\")\n",
    "nearest_similarity_cosmul(\"Ron\", \"Potter\", \"Hermione\")\n",
    "nearest_similarity_cosmul(\"Dumbledore\", \"McGonagall\", \"Sirius\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
